---
title: "ST694 Project"
author: "Douglas Bowen"
date: '2022-03-31'
output: html_document
---

```{r, include=FALSE, echo=FALSE, message=FALSE}
#Setup Chunk
knitr::opts_chunk$set(echo = TRUE,tidy.opts=list(width.cutoff=80),tidy=TRUE)

#Required Libraries for Course
library(ISLR2)
library(class)
library(boot)
library(glmnet) #Added A2/A3
library(pls) #Added A2/A3
library(splines) #Added A3
library(leaps)#Added A3
library(gam) #Added A3
#remove.packages(c("tree","randomForest"))
#install.packages(c("tree","randomForest"))
library(tree) #Added A4
library(randomForest) #Added A4

#Optional Libraries Not Recommended By Course
#Libraries for Graphing
#library(tidyverse)
library(scales)

#Other Useful Libraries (for R Markdown)
library(knitr)
library(kableExtra)
library(latex2exp)
library(formatR)

#Old Packages from Prior Courses
library(multcomp)
library(multcompView)
library(pander)
library(MASS)
library(tinytex)


#install.packages(c("ISLR2","class","boot","tidyverse","scales","knitr","kableExtra","latex2exp","multcomp","multcompview","pander","MASS","gridExtra","leaps","bestglm","corrplot"))

#Notes:
#Use "install.packages('library_name_here')" then use "library(library_name_here)"
```

# Loading Data

```{r, include=FALSE, echo=FALSE, message=FALSE}

#test <- read.csv("test.csv")
train <- read.csv("Data/train.csv")

#Remove ID Column (Useless)
train$Id = NULL

#Begin by examining NA values
Tot_NA <- sum(is.na(train))/prod(dim(train)) #Total NA Values

Col_NA <- sort(colMeans(is.na(train)), decreasing=TRUE) #Column NA Values
kable(Col_NA[which(Col_NA>0)], caption="Column NA Rates", col.names = c("NA Percentage")) #Column NA Values > 0%

#Remove NA Columns > Threshold
threshold_na = 0.4
Col_Remove = names(Col_NA[which(Col_NA>threshold_na)])
Col_Impute = names(Col_NA[which(Col_NA<=threshold_na & Col_NA>0)])

#Generating Subset of Data
#test <- test[,which(!names(test) %in% Col_Remove)]
train <- train[,which(!names(train) %in% Col_Remove)]

#Mode Imputation Function
getmode <- function(v) {
   uniqv <- unique(v[!is.na(v)])
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

#Fill in Remaining NA Data - Use Mean Method since Large Dataset
#Examine Data
for (column in Col_Impute){
  col_dat = train[,column]
  hist(col_dat, main=column, sub=class(col_dat))
  
  if (class(col_dat) == "numeric"){
      train[is.na(col_dat),column] = mean(col_dat, na.rm=TRUE)
      
  } else if (class(col_dat) == "integer"){
      train[is.na(col_dat),column] = getmode(col_dat)
      
  } else{
      print("Not Accounted For Yet")
  }
}

#Examine Correlation Matrix for Colinearity Prior to Analysis (maybe move above)
threshold_corr = 0.7
corr_check <- cor(train[,which(!sapply(train, class) %in% "character")], use = "complete.obs")
(corr_check>threshold_corr & corr_check != 1)
#high_corr <- corr_check[corr_check>threshold_corr & corr_check != 1]

#Still need to pull what colname/rowname those values associate with - how?

#Once correlations determined know to only use 1 of 2 most likely (to avoid Multi-Collinearity Issues)
#Or, can utilize PCA to deal with this instead.







#Export to Cleaned Train CSV Set
train_y = subset(train, select=c(Response))
train_x = subset(train, select=-c(Response))

write.csv(train,"Data/train_clean.csv", row.names = FALSE)
write.csv(train_y,"Data/train_y_clean.csv", row.names = FALSE)
write.csv(train_x,"Data/train_x_clean.csv", row.names = FALSE)

```





















